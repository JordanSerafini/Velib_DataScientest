---
project: "Projet DPM ‚Äì V√©lib' Paris"
team: "Jordan S., Abdelmalek B."
formation: "RNCP 7 ‚Äì Machine Learning & IA ‚Äì DataScientest"
period: "Sept.‚ÄìNov. 2025"
role: "Data Product Manager (bin√¥me)"
---

# 1.3.1 ‚Äî Priorisation des am√©liorations

## Contexte & Objectifs

**Situation** : La Discovery (sections 1.2.1 et 1.2.2) a identifi√© 3 probl√®mes principaux. Maintenant, **par quoi commencer ?**

**Objectif** : Utiliser une m√©thode rigoureuse de priorisation pour justifier nos choix.

**Notre approche** : M√©thode Impact √ó Effort (inspir√©e du framework RICE enseign√© chez DataScientest).

---

## Les 3 probl√®mes identifi√©s en Discovery

**P1 - Stations critiques aux heures de pointe**
- **Constat** : Stock-out (0 v√©lo) et dock-out (0 borne libre) fr√©quents
- **Cons√©quences** : D√©tours, frustration, perte de temps

**P2 - R√©gulation peu efficace**
- **Constat** : R√©√©quilibrage r√©actif, pas pr√©dictif
- **Cons√©quences** : Co√ªts logistiques √©lev√©s, efficacit√© variable

**P3 - Gestion des anomalies mal v√©cue**
- **Constat** : Parcours de r√©clamation pas optimal
- **Cons√©quences** : Tickets support, exp√©rience d√©grad√©e

---

## Transformation en am√©liorations

**A1 - R√©duire la criticit√© des stations aux heures de pointe**
- **Objectif** : -20% SOR/DOR sur Top 100 stations
- **Approche** : Alerting + ajustements capacit√© cibl√©s

**A2 - Optimiser les tourn√©es de r√©gulation**
- **Objectif** : +10-15 points efficacit√© (RE)
- **Approche** : Priorisation data-driven, algorithme optimisation

**A3 - Am√©liorer la gestion des anomalies**
- **Objectif** : -15% tickets support, -10% MTTR
- **Approche** : Diagnostic guid√© dans l'app, FAQ contextuelle

**A4 - Recommander des alternatives aux usagers**
- **Objectif** : -10% JAR (Journey Abandon Rate), -15% d√©tours
- **Approche** : Suggestions stations proches + probabilit√© dispo

---

## M√©thode de priorisation : Impact √ó Effort

### Principe

```
Score = Impact / (Effort + 1)
```

Plus le score est √©lev√©, plus l'am√©lioration est prioritaire.

### √âchelle de notation

**Impact** (0 √† 5) :
- Combine impact business ET impact utilisateur
- 5 = impact majeur, 0 = impact nul

**Effort** (0 √† 5) :
- Complexit√© technique + qualit√© donn√©es + contraintes orga
- 5 = tr√®s difficile, 0 = trivial

### Crit√®res d√©taill√©s

**Impact** (5 crit√®res) :
- I1 - Acquisition/R√©tention usagers
- I2 - Satisfaction client (NPS)
- I3 - Efficience op√©rationnelle (r√©duction co√ªts)
- I4 - R√©duction risques/Conformit√©
- I5 - Alignement strat√©gique

**Effort** (3 crit√®res) :
- F1 - Complexit√© technique
- F2 - Disponibilit√©/Qualit√© donn√©es
- F3 - D√©pendances organisationnelles

---

## Scoring des 4 am√©liorations

**Pond√©rations choisies** (inspir√©es du cours DPM DataScientest) :

**Impact** :
- I1 = 0.25, I2 = 0.30, I3 = 0.30, I4 = 0.10, I5 = 0.05

**Effort** :
- F1 = 0.5, F2 = 0.3, F3 = 0.2

### R√©sultats

| Am√©lioration | Impact pond√©r√© | Effort pond√©r√© | Score final |
|--------------|---------------|---------------|-------------|
| **A1 - R√©duire criticit√© stations** | 3.73 | 1.75 | **1.36** ü•á |
| **A3 - Fluidifier anomalies** | 2.63 | 1.40 | **1.09** ü•à |
| **A2 - Optimiser r√©gulation** | 3.33 | 2.50 | **0.95** ü•â |
| **A4 - Reco station + proba** | 2.98 | 2.15 | **0.95** ü•â |

### Interpr√©tation

**ü•á A1 en t√™te** (1.36)
- Fort impact sur satisfaction (4.5/5) ET efficience (4.0/5)
- Effort mod√©r√© (donn√©es GBFS disponibles)

**ü•à A3 en 2√®me** (1.09) : quick win UX
- Peu de complexit√© technique (am√©lioration app)
- Impact satisfaisant sur satisfaction client

**ü•â A2 et A4 en 3√®me** (0.95) : plus techniques
- Complexit√© plus √©lev√©e (algo optimisation, ML)
- N√©cessitent plus de travail data

**Limite** : Ce scoring est **subjectif**. Les notes ont √©t√© attribu√©es par nous (bin√¥me), pas valid√©es avec les √©quipes Smovengo. Dans un vrai projet DPM, il faudrait un atelier de priorisation avec les parties prenantes.

---

## Feuille de route recommand√©e

Sur la base du scoring :

### Priorit√© 1 : A1 - R√©duire criticit√© stations (4-6 semaines)

**Livrables** :
- Dashboard BI avec KPIs SOR/DOR
- Syst√®me d'alerting (Slack/Email si SOR > 40%)
- Vue Top stations critiques par cr√©neau (8h-10h / 18h-20h)

**MVP** :
- Dashboard avec mise √† jour quotidienne
- Alertes pour Top 10 stations critiques

**Crit√®re de succ√®s** : **-20% minutes en √©tat critique** (vs baseline)

### Priorit√© 2 : A3 - Am√©liorer gestion anomalies (3-4 semaines)

**Livrables** :
- Diagnostic guid√© dans l'app ("V√©lo bloqu√© ? Suivez ces 3 √©tapes...")
- FAQ contextuelle

**Crit√®re de succ√®s** : **-15% tickets support** li√©s aux blocages

### Priorit√© 3 : A2 - Optimiser tourn√©es r√©gulation (8-10 semaines)

**Livrables** :
- Mod√®le pr√©vision demande (baseline : moyenne mobile)
- Syst√®me scoring pour prioriser stations
- Algo heuristique optimisation tourn√©es

**Crit√®re de succ√®s** : **+10 points RE** (Rebalancing Efficiency)

### Priorit√© 4 : A4 - Recommandations stations alternatives (6-8 semaines)

**Livrables** :
- Calcul probabilit√© dispo √† H+15
- Reco stations alternatives (rayon 400m)
- Int√©gration app mobile

**Crit√®re de succ√®s** : **-10% JAR** (Journey Abandon Rate)

---

## P√©rim√®tre & Hors-p√©rim√®tre

### Dans le p√©rim√®tre
‚úÖ Priorisation m√©thodologique (Impact/Effort)
‚úÖ D√©finition des am√©liorations
‚úÖ Feuille de route recommand√©e

### Hors p√©rim√®tre
‚ùå Validation du scoring avec √©quipes Smovengo
‚ùå Impl√©mentation des am√©liorations (on est en phase Conception)
‚ùå Ajustement dynamique de la priorisation

---

## Limites & Risques

### Limites identifi√©es
1. **Scoring subjectif** : Notes attribu√©es par nous (bin√¥me), pas valid√©es avec parties prenantes.
2. **Hypoth√®ses non test√©es** : Ex. on suppose que l'effort de A1 est mod√©r√© (1.75/5), mais on n'a pas fait d'√©tude de faisabilit√© technique d√©taill√©e.
3. **Pas de validation utilisateurs** : On n'a pas pu pr√©senter ce scoring aux √©quipes Smovengo pour validation.
4. **Pond√©rations arbitraires** : Les poids (I1=0.25, etc.) sont inspir√©s du cours, mais pas adapt√©s au contexte sp√©cifique V√©lib'.

### Risques
- **Biais confirmation** : On peut √™tre tent√© de scorer plus haut l'am√©lioration qu'on pr√©f√®re.
- **Sur-estimation de l'impact** : Ex. on dit "-20% SOR", mais c'est peut-√™tre optimiste.
- **Sous-estimation de l'effort** : A1 peut √™tre plus complexe qu'on ne le pense (qualit√© donn√©es, bugs API GBFS, etc.).

---

## Impact & Next Steps

### Impact attendu
Si on suit cette priorisation :
- **Focus** sur l'am√©lioration √† fort impact et effort mod√©r√© (A1)
- **Quick win** avec A3 (peu d'effort, impact satisfaisant)
- **Patience** pour A2 et A4 (plus techniques, √† faire apr√®s)

### Next Steps
1. **Section 1.3.2** : D√©finir les KPIs pour mesurer A1
2. **Section 1.3.3** : Cr√©er le Product Vision Board pour A1
3. **DPM-2** : Concevoir la solution (ML, BI)

---

## Hypoth√®ses √† valider

**H1** : A1 a vraiment un effort mod√©r√© (1.75/5)
**Validation** : √âtude de faisabilit√© technique (acc√®s GBFS, qualit√© donn√©es, partitioning PostgreSQL)

**H2** : Les pond√©rations (I1=0.25, etc.) sont adapt√©es au contexte V√©lib'
**Validation** : Atelier avec parties prenantes (direction, √©quipes ops, produit)

**H3** : -20% SOR est atteignable en 6 semaines
**Validation** : Benchmarks internationaux (Citi Bike, Santander Cycles)

---

## Sources & R√©f√©rences

**M√©thodologie** :
- Cours DataScientest DPM : Priorisation, RICE framework
- Article : "How to prioritize features" (Product School)

**Scoring** :
- Inspir√© du framework RICE (Reach √ó Impact √ó Confidence / Effort)
- Adapt√© avec crit√®res Impact (I1-I5) et Effort (F1-F3)

