---
project: "Projet DPM ‚Äì V√©lib' Paris"
team: "Jordan S., Abdelmalek B."
formation: "RNCP 7 ‚Äì Machine Learning & IA ‚Äì DataScientest"
period: "Sept.‚ÄìNov. 2025"
role: "Data Product Manager (bin√¥me)"
rewritten_for: "Soutenance orale ‚Äì version √©tudiante plausible"
---

# 2.1.1 ‚Äî M√©thodologie conception produit ML

## Contexte & Objectifs

**Situation** : On a identifi√© un besoin (Epic A2 - Optimiser r√©gulation). Maintenant, **comment passer de "on va pr√©dire la demande" √† un vrai produit utilisable ?**

**Objectif** : Fournir une m√©thodologie pour transformer une id√©e ML en produit fonctionnel.

**Pourquoi ?** Pour √©viter le pi√®ge classique : "On entra√Æne un mod√®le... et apr√®s ?"

---

## 1. Sp√©cifier le cas d'usage (5 questions)

Avant de coder quoi que ce soit, r√©pondre √† 5 questions :

### Q1 : Qui utilise les pr√©dictions ?
- Utilisateur : R√©gulateurs terrain (15-20 personnes)
- Profil : peu technique, smartphone, en mouvement
- Fr√©quence : 4-6 fois/shift

### Q2 : √Ä quoi servent les pr√©dictions ?
- **Action** : D√©cider quelles stations r√©guler en priorit√©
- **Impact** : R√©duire SOR/DOR
- **Criticit√©** : Fausse pr√©diction ‚Üí d√©placement inutile (~20-30‚Ç¨)

### Q3 : Comment s'int√®grent-elles dans le workflow ?

**AVANT (sans ML)** :
1. R√©gulateur re√ßoit liste fixe de stations
2. Suit tourn√©e, sans priorisation dynamique
3. Arrive trop tard ou trop t√¥t

**APR√àS (avec ML)** :
1. Ouvre app mobile √† 9h
2. Voit carte avec pr√©dictions criticit√© √† 30 min (code couleur)
3. App propose itin√©raire optimis√©
4. Valide ‚Üí itin√©raire envoy√© au GPS

### Q4 : Quelle forme prennent les pr√©dictions ?
- **Format** : Classe criticit√© (üü¢ Normal / üü† Attention / üî¥ Critique) + score confiance (0-100%)
- **Fr√©quence** : Mise √† jour toutes les 10 min
- **Horizon** : Pr√©diction √† +30 min

### Q5 : Comment mesurer le succ√®s ?
- **KPI m√©tier** : -30% SOR/DOR en 6 mois, RE de 65% √† 80%
- **KPI technique** : Pr√©cision ‚â• 75%, Rappel ‚â• 70%

**Checklist** : Si une seule question n'a pas de r√©ponse claire ‚Üí **STOP**, ne pas passer √† l'√©tape suivante. Retourner voir les utilisateurs.

---

## 2. Cahier des charges ML (8 composantes)

### 2.1 T√¢che ML
**Type** : Classification multi-classe (üü¢ / üü† / üî¥)

**Pourquoi pas r√©gression ?** R√©gulateur veut juste savoir "je dois y aller maintenant ?" (signal actionnable).

### 2.2 Variable cible (Target)
```
criticality_level_30min:
- 0 = üü¢ Normal (‚â• 3 v√©los ET ‚â• 3 bornes)
- 1 = üü† Attention (1-2 v√©los OU 1-2 bornes)
- 2 = üî¥ Critique (0 v√©lo OU 0 borne)
```

**Labellisation** : Utiliser donn√©es historiques `fct_station_snapshot`, observer √©tat r√©el √† t+30min.

### 2.3 Donn√©es d'entr√©e (Input Data)
**Disponibles au moment t** :
- √âtat actuel station (nb v√©los, nb bornes)
- √âtat stations voisines (rayon 500m)
- M√©t√©o actuelle (temp√©rature, pluie, vent)
- Temporel (heure, jour semaine, f√©ri√©, gr√®ve)
- Historique agr√©g√© (demande moy 7 derniers jours)

**‚ö†Ô∏è PAS disponible** : Donn√©es √† t+10min, t+20min, t+30min (le futur !)

### 2.4 Features (Engineering)
**Exemples** :
- Temporelles : hour, day_of_week, is_weekend, is_rush_hour
- √âtat station : bikes_available, occupancy_rate, turnover_1h
- Voisinage : avg_bikes_nearby_500m
- Historique : bikes_7d_same_hour_avg

**Nombre** : ~30-50 features

### 2.5 Contraintes performance
- **Latence** : < 200ms par pr√©diction (100 stations)
- **Throughput** : 10 pr√©dictions/sec
- **Pr√©cision** : ‚â• 75% (classe Critique)
- **Rappel** : ‚â• 70% (ne pas louper trop de stations critiques)

### 2.6 Collecte & labellisation
- **Source** : GBFS historis√© (d√©j√† dispo)
- **Labellisation** : Automatique via SQL (LEAD function)
- **Volume** : ~2-3 millions lignes (6 mois historique)

### 2.7 √âvaluation
- **M√©trique** : Precision, Recall, F1-score (par classe)
- **Validation** : Temporal split (80% train, 20% test)
- **Baseline** : Moyenne mobile 7 jours ‚Üí Accuracy ~68%
- **Objectif** : Accuracy ‚â• 80% (gain +12 points)

### 2.8 Monitoring & Retraining
- **Monitoring** : Distribution pr√©dictions, drift detection
- **Retraining** : Mensuel (ou si performance < 70%)
- **Alerting** : Email si accuracy < 75% pendant 3 jours

---

## 3. Choix de l'algorithme (MVM)

**Principe MVM (Minimum Viable Model)** : Commencer par le plus simple.

| Niveau | Approche | Accuracy estim√©e | Effort | D√©cision |
|--------|----------|------------------|--------|----------|
| 0 | R√®gle statique ("< 3 v√©los = critique") | ~60% | 1 jour | ‚ùå Trop faible |
| 2 | Moyenne mobile 7j | ~68% | 2 jours | ‚ùå Pas assez |
| 3 | LightGBM (40 features) | ~80% | 4 semaines | ‚úÖ **MVP** |
| 4 | LSTM (r√©seau neurones) | ~82% | 8 semaines | ‚ùå Gain marginal |

**Choix** : LightGBM (niveau 3) = meilleur compromis.

---

## P√©rim√®tre & Hors-p√©rim√®tre

### Dans le p√©rim√®tre
‚úÖ M√©thodologie conception produit ML
‚úÖ Cahier des charges ML (8 composantes)
‚úÖ Choix algorithme (MVM)

### Hors p√©rim√®tre
‚ùå Impl√©mentation du mod√®le (pas fait pour projet √©tudiant)
‚ùå Feature engineering d√©taill√©
‚ùå Tuning hyperparam√®tres
‚ùå D√©ploiement en production (API REST)

---

## Limites & Risques

### Limites identifi√©es
1. **Pas de donn√©es r√©elles** : On n'a pas historis√© GBFS ‚Üí pas de dataset pour entra√Æner le mod√®le.
2. **Hypoth√®ses non test√©es** : On suppose que 40 features suffisent, mais on n'a pas test√©.
3. **Pas de validation users** : On n'a pas pu pr√©senter ce workflow aux r√©gulateurs (Julien).

### Risques
- **Overfitting** : Mod√®le trop ajust√© aux donn√©es historiques ‚Üí mauvaise g√©n√©ralisation.
- **Data leakage** : Si on utilise accidentellement des donn√©es du futur dans les features.
- **Adoption** : M√™me si le mod√®le est bon, il faut que les r√©gulateurs l'utilisent.

---

## Impact & Next Steps

### Impact attendu
Si on impl√©mentait ce produit ML :
- **R√©gulateurs** : Tourn√©es optimis√©es, moins de d√©placements inutiles
- **Op√©rateurs** : RE de 65% √† 80% (+15 points)
- **Usagers** : Moins de stock-out/dock-out

### Next Steps
1. **Collecter donn√©es** : Historiser GBFS pendant 3-6 mois
2. **Feature engineering** : Cr√©er les 30-50 features
3. **Entra√Ænement** : Tester LightGBM vs autres algos
4. **√âvaluation** : Validation temporelle (pas al√©atoire !)
5. **D√©ploiement** : API REST + int√©gration app mobile

